{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二章\n",
    "如果重排OPEN表是依据发f(x)=g(x)+h(x)进行的,则称该过程为\n",
    "A、A*算法\n",
    "B、A算法\n",
    "C、有序搜索\n",
    "D、启发式搜索\n",
    "正确答案： B 我的答案：B\n",
    "2\n",
    "应用某个算法选择OPEN表上具有最小f值的节点作为下一个要扩展的节点。这种搜索方法的算法就叫做\n",
    "A、盲目搜索\n",
    "B、宽(广)度优先搜索策略\n",
    "C、有序搜索算法\n",
    "D、极小极大分析法\n",
    "正确答案： C 我的答案：C得分： 5.0分\n",
    "3\n",
    "宽(广)度优先搜索方法能够保证在搜索树中找到一条通往目标节点()途径(如果有路径存在时)\n",
    "A、可行\n",
    "B、最短\n",
    "C、最长\n",
    "D、解答\n",
    "正确答案： B 我的答案：B得分： 5.0分\n",
    "4\n",
    "如果问题存在最优解,则下面几种搜索算法中,()必然可以得到该最优解。\n",
    "A、深度优先搜索\n",
    "B、宽(广)度优先搜索\n",
    "C、有序搜索算法\n",
    "D、启发式搜索\n",
    "正确答案： B 我的答案：B得分： 5.0分\n",
    "5\n",
    "下列哪种搜索算法中是利用问题拥有的启发信息来引导搜索,达到减少搜索范围、降低问题复杂度的目的()\n",
    "A、深度优先搜索\n",
    "B、宽(广)度优先搜索\n",
    "C、盲目搜索算法\n",
    "D、启发式搜索\n",
    "正确答案： D 我的答案：D得分： 5.0分\n",
    "二.多选题（共5题,25.0分）\n",
    "1\n",
    "下面属于盲目搜索的方法有()\n",
    "A、深度优先搜索\n",
    "B、宽(广)度优先搜索\n",
    "C、有序搜索算法\n",
    "D、启发式搜索\n",
    "正确答案： AB 我的答案：AB得分： 5.0分\n",
    "2\n",
    "A*算法是一种()\n",
    "A、图搜索策略\n",
    "B、有序搜索策略\n",
    "C、盲目搜索\n",
    "D、启发式搜索\n",
    "正确答案： ABD 我的答案：ABD得分： 5.0分\n",
    "3\n",
    "根据估价函数值,按由小到大的次序对Open表中的节点进行重新排序的搜索方法叫做()\n",
    "A、深度优先搜索\n",
    "B、宽(广)度优先搜索\n",
    "C、有序搜索算法\n",
    "D、启发式搜索\n",
    "正确答案： CD 我的答案：CD得分： 5.0分\n",
    "4\n",
    "Dijkstra算法的特点有()\n",
    "A、效率高\n",
    "B、效率低\n",
    "C、路径最短\n",
    "D、复杂度高\n",
    "正确答案： BCD 我的答案：BC得分： 2.5分\n",
    "5\n",
    "下面属于宽(广)度优先搜索和深度优先搜索具有的相同点是()\n",
    "A、属于最短路径算法\n",
    "B、效率低\n",
    "C、搜索路线\n",
    "D、复杂度高\n",
    "正确答案： ABD 我的答案：BD得分： 2.5分\n",
    "三.判断题（共10题,50.0分）\n",
    "1\n",
    "启发式搜索中,通常OPEN表上的节点按照他们的f函数值递减顺序排列\n",
    "我的答案：× 得分： 5.0分正确答案：×\n",
    "2\n",
    "启发式搜索是一种利用启发式信息的搜索\n",
    "我的答案：√ 得分： 5.0分正确答案：√\n",
    "3\n",
    "图搜索算法中,CLOSE表用来登记待考察的节点\n",
    "我的答案：× 得分： 5.0分正确答案：×\n",
    "4\n",
    "如果搜索是以接近起始节点的程度依次扩展节点的,那么这种搜索就叫做宽(广)度优先搜索\n",
    "我的答案：√ 得分： 5.0分正确答案：√\n",
    "5\n",
    "Dijkstra算法是典型的宽度优先搜索算法,该算法路径最短、效率最高\n",
    "我的答案：× 得分： 5.0分正确答案：×\n",
    "6\n",
    "在进行有序搜索法中,此时Open表是一个按节点的启发估价函数值的大小为序排列的一个优先队列\n",
    "我的答案：√ 得分： 5.0分正确答案：√\n",
    "7\n",
    "启发式搜索一定比盲目式搜索好\n",
    "我的答案：× 得分： 5.0分正确答案：×\n",
    "8\n",
    "宽(广)度优先搜索策略是一种完整的搜索策略,只要问题有解,就能找到解\n",
    "我的答案：√ 得分： 5.0分正确答案：√\n",
    "9\n",
    "深度优先搜索策略可能找不到最优解,也可能根本找不到解\n",
    "我的答案：√ 得分： 5.0分正确答案：√\n",
    "10\n",
    "因为估值函数最优,所以A*算法是最优的A算法\n",
    "我的答案：√ "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第三章：\n",
    "华东理工大学本科\n",
    "肖婷婷\n",
    "\n",
    "首页任务统计资料通知作业考试案例教学讨论\n",
    "人工智能基础  \n",
    "返回\n",
    "第3章\n",
    "   姓名：肖婷婷   班级：动181-3   成绩： 81.4分\n",
    "\n",
    "一.单选题（共4题,24.8分）\n",
    "1\n",
    "关于蚁群算法 ,说法不正确的是\n",
    "A、是最简单的进化算法\n",
    "B、是群体智能的典型算法\n",
    "C、模拟了蚂蚁群体解决问题的方法\n",
    "D、其灵感来源于蚁群在觅食过程中,总能找到蚁穴到食物之间最短路径的现象\n",
    "正确答案： A 我的答案：A得分： 6.2分\n",
    "2\n",
    "以下关于蚁群算法的说法,错误的是\n",
    "A、蚁群算法用于预测蚁群在给定条件下的行为\n",
    "B、蚁群算法是从生物智能现象抽象出的算法\n",
    "C、蚁群算法属于群体智能\n",
    "D、对蚁群觅食的模拟要考虑到环境、选择路线等等多种因素\n",
    "正确答案： A 我的答案：A得分： 6.2分\n",
    "3\n",
    "关于动态规划算法的特点,不正确的是\n",
    "A、使用最优化原理\n",
    "B、完备搜索\n",
    "C、无后效性\n",
    "D、有重叠子问题\n",
    "正确答案： B 我的答案：C得分： 0.0分\n",
    "4\n",
    "动态规划方法的解题步骤不包括\n",
    "A、分析最优解的性质,并刻画其结构特征\n",
    "B、递归的定义最优解\n",
    "C、确定子过程指标函数的具体形式\n",
    "D、根据计算最优值时得到的信息,构造问题的最优解\n",
    "正确答案： C 我的答案：C得分： 6.2分\n",
    "二.多选题（共4题,24.8分）\n",
    "1\n",
    "以下关于蚁群算法说法正确的是\n",
    "A、蚁群算法是一种自组织的算法\n",
    "B、蚁群算法是一种本质上并行的算法\n",
    "C、蚁群算法是一种正反馈的算法\n",
    "D、蚁群算法具有较强的鲁棒性\n",
    "正确答案： ABCD 我的答案：ABCD得分： 6.2分\n",
    "2\n",
    "在蚁群算法中,信息素启发因子反映了蚁群在路径搜索中随机性因素的作用。其值越大,则\n",
    "A、蚂蚁选择以前走过的路径的可能性就越大\n",
    "B、蚂蚁选择以前走过的路径的可能性就越小\n",
    "C、算法搜索的随机性越大,搜索的收敛速度会加快\n",
    "D、算法搜索的随机性越小,搜索结果易陷于局部最优\n",
    "正确答案： BD 我的答案：AD得分： 0.0分\n",
    "3\n",
    "动态规划问题包括\n",
    "A、离散确定性动态规划问题\n",
    "B、线性动态规划问题\n",
    "C、非线性动态规划问题\n",
    "D、连续随机性动态规划问题\n",
    "正确答案： AD 我的答案：ABD得分： 0.0分\n",
    "4\n",
    "动态规划法求解问题包括()阶段\n",
    "A、划分阶段\n",
    "B、确定状态和状态变量\n",
    "C、确定决策并写出状态转移方程\n",
    "D、寻找边界条件\n",
    "正确答案： ABCD 我的答案：ABCD得分： 6.2分\n",
    "三.判断题（共8题,50.4分）\n",
    "1\n",
    "蚁群算法是一种应用于组合优化问题的启发式搜索算法。\n",
    "我的答案：√ 得分： 6.2分正确答案：√\n",
    "2\n",
    "蚁群算法是通过人工模拟蚂蚁搜索食物的过程,即通过个体之间的信息交流与相互协作最终找到从蚁穴到食物源的最短路径的。\n",
    "我的答案：√ 得分： 6.2分正确答案：√\n",
    "3\n",
    "蚁群算法中,蚂蚁选择路径的原理是一种负反馈机制。\n",
    "我的答案：× 得分： 6.2分正确答案：×\n",
    "4\n",
    "蚂蚁系统是一种增强型学习系统。\n",
    "我的答案：√ 得分： 6.2分正确答案：√\n",
    "5\n",
    "动态规划阶段的顺序不同,则结果不同。\n",
    "我的答案：× 得分： 6.2分正确答案：×\n",
    "6\n",
    "状态是由决策确定的。\n",
    "我的答案：× 得分： 6.2分正确答案：×\n",
    "7\n",
    "用逆序法求解动态规划问题的重要基础之一是最优性原理。\n",
    "我的答案：√ 得分： 6.2分正确答案：√\n",
    "8\n",
    "列表法是求解某些离散变量动态规划问题的有效方法。\n",
    "我的答案：√ 得分： 7.0分正确答案：√"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第四章：\n",
    "华东理工大学本科\n",
    "肖婷婷\n",
    "\n",
    "首页任务统计资料通知作业考试案例教学讨论\n",
    "人工智能基础  \n",
    "返回\n",
    "第4章\n",
    "   姓名：肖婷婷   班级：动181-3   成绩： 83.6分\n",
    "\n",
    "一.单选题（共5题,38.0分）\n",
    "1\n",
    "一个支持概念学习的算法不需要包括以下哪个部分：\n",
    "A、训练数据\n",
    "B、目标概念\n",
    "C、划分标准\n",
    "D、实际数据对象\n",
    "正确答案： C 我的答案：C得分： 7.6分\n",
    "2\n",
    "使用Finds算法，根据如下判断性别的表格，假设所有男性为正例，得到一般假设为： \n",
    "A、<yes,?,?,low>\n",
    "B、<yes,?,high,low>\n",
    "C、<yes,short,?,low>\n",
    "D、<yes,short,high,low>\n",
    "正确答案： A 我的答案：A得分： 7.6分\n",
    "3\n",
    "下列关于候选消除算法，说法不正确的是：\n",
    "A、候选消除算法能够表示与训练样例一致的所有假设。\n",
    "B、关于假设空间H和训练数据D的一般边界G，是在H中与D相一致的最一般成员的集合。\n",
    "C、关于假设空间H和训练数据D的特殊边界S，是在H中与D相一致的最特殊成员的集合。\n",
    "D、候选消除算法首先将G集合初始化为H中最特殊假设，将S集合初始化为H中最一般假设。\n",
    "正确答案： D 我的答案：D得分： 7.6分\n",
    "4\n",
    "决策树的构成顺序是：\n",
    "A、特征选择、决策树生成、决策树剪枝。\n",
    "B、决策树剪枝、特征选择、决策树生成。\n",
    "C、决策树生成、决策树剪枝、特征选择。\n",
    "D、特征选择、决策树剪枝、决策树生成。\n",
    "正确答案： A 我的答案：A得分： 7.6分\n",
    "5\n",
    "决策树法是运用（ ）来分析和选择决策方案的一种系统分析方法。\n",
    "A、线形图\n",
    "B、树状图\n",
    "C、饼图\n",
    "D、柱状图\n",
    "正确答案： B 我的答案：B得分： 7.6分\n",
    "二.判断题（共6题,45.6分）\n",
    "1\n",
    "概念学习可以看作是一个搜索问题的过程。\n",
    "我的答案：√ 得分： 7.6分正确答案：√\n",
    "2\n",
    "Finds算法缺点是无法确定是否找到了唯一合适的假设，并且容易受到噪声的影响。\n",
    "我的答案：√ 得分： 7.6分正确答案：√\n",
    "3\n",
    "变型空间表示了目标概念的所有合理的变型。\n",
    "我的答案：√ 得分： 7.6分正确答案：√\n",
    "4\n",
    "候选消除算法无法输出与训练样例一致的所有假设集合。\n",
    "我的答案：× 得分： 7.6分正确答案：×\n",
    "5\n",
    "决策树分析法是通过决策树图形展示重要结局，明确思路，比较各种备选方案预期结果进行决策的方法。\n",
    "我的答案：√ 得分： 7.6分正确答案：√\n",
    "6\n",
    "决策树只能进行单级决策。\n",
    "我的答案：× 得分： 7.6分正确答案：×\n",
    "三.简答题（共2题,16.4分）\n",
    "1\n",
    "运用候选消除算法，根据下表中的物品的大小、颜色、形状各属性，预测是否需要购买。 \n",
    "正确答案：\n",
    "\n",
    "我的答案：\n",
    "\n",
    "\n",
    "\n",
    "2\n",
    "运用决策树算法，根据下列动物数据表，判断动物是否为鱼类。其中no surfacing指不浮出水面能否生存，flipper指是否有脚。 \n",
    "正确答案：\n",
    "\n",
    "我的答案：\n",
    "\n",
    "#coding=utf-8\n",
    "\n",
    "import operator\n",
    "\n",
    "from math import log\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "\n",
    "    numEntries = len(dataSet)\n",
    "\n",
    "    labelCounts = {}\n",
    "\n",
    "    for featVec in dataSet:\n",
    "\n",
    "        currentLabel = featVec[-1]\n",
    "\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "\n",
    "            labelCounts[currentLabel] = 0\n",
    "\n",
    "        labelCounts[currentLabel] += 1\n",
    "\n",
    "    shannonEnt = 0.0\n",
    "\n",
    "    for key in labelCounts:\n",
    "\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet():\n",
    "\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "\n",
    "               [1, 1, 'yes'],\n",
    "\n",
    "               [1, 0, 'no'],\n",
    "\n",
    "               [0, 1, 'no'],\n",
    "\n",
    "               [0, 1, 'no']]\n",
    "\n",
    "    labels = ['no sufacing', 'flippers']\n",
    "\n",
    "    return dataSet, labels\n",
    "\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "\n",
    "    retDataSet = []\n",
    "\n",
    "    for featVec in dataSet:\n",
    "\n",
    "        if featVec[axis] == value:\n",
    "\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "\n",
    "    return retDataSet\n",
    "\n",
    "myDat, labels = createDataSet()\n",
    "\n",
    "def majorityCnt(classList):\n",
    "\n",
    "    classCount = {}\n",
    "\n",
    "    for vote in classList:\n",
    "\n",
    "        if vote not in classCount.keys(): classCount[vote] = 0\n",
    "\n",
    "        classCount[vote] += 1\n",
    "\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "\n",
    "    bestInfoGain = 0.0\n",
    "\n",
    "    bestFeature = -1\n",
    "\n",
    "    for i in range(numFeatures):\n",
    "\n",
    "        featList = [example[i] for example in dataSet]\n",
    "\n",
    "        uniqueVals = set(featList)\n",
    "\n",
    "        newEntropy = 0.0\n",
    "\n",
    "        for value in uniqueVals:\n",
    "\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "\n",
    "        if (infoGain > bestInfoGain):\n",
    "\n",
    "            bestInfoGain = infoGain\n",
    "\n",
    "            bestFeature = i\n",
    "\n",
    "    return bestFeature\n",
    "\n",
    "myData, labels = createDataSet()\n",
    "\n",
    "chooseBestFeatureToSplit(myDat)\n",
    "\n",
    "def createTree(dataSet, labels):\n",
    "\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "\n",
    "        return classList[0]\n",
    "\n",
    "    if len(dataSet[0]) == 1:\n",
    "\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "\n",
    "    del(labels[bestFeat])\n",
    "\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "\n",
    "    uniqueVals = set(featValues)\n",
    "\n",
    "    for value in uniqueVals:\n",
    "\n",
    "        subLabels = labels[:] \n",
    "\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # 递归构造子树\n",
    "\n",
    "    return myTree\n",
    "\n",
    "myDat, labels =createDataSet()\n",
    "\n",
    "myTree = createTree(myDat, labels)\n",
    "\n",
    "print(myTree)\n",
    "\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "\n",
    "    secondDict = inputTree[firstStr]\n",
    "\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "\n",
    "    for key in secondDict.keys():\n",
    "\n",
    "        if testVec[featIndex] == key:\n",
    "\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "\n",
    "            else:\n",
    "\n",
    "                classLabel = secondDict[key]\n",
    "\n",
    "    return classLabel\n",
    "\n",
    "myDat, labels = createDataSet()\n",
    "\n",
    "myTree = createTree(myDat, labels[:])\n",
    "\n",
    "print(classify(myTree, labels, [1, 0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第一章：\n",
    "华东理工大学本科\n",
    "肖婷婷\n",
    "\n",
    "首页任务统计资料通知作业考试案例教学讨论\n",
    "人工智能基础  \n",
    "返回\n",
    "第1章\n",
    "   姓名：肖婷婷   班级：动181-3   成绩： 63分\n",
    "\n",
    "一.单选题（共1题,9.0分）\n",
    "1\n",
    "目前发展势头最猛、风头最盛的（ ）、深度神经网络，即属于联结主义。\n",
    "A、深度学习\n",
    "B、知识图谱\n",
    "C、专家系统\n",
    "正确答案： A 我的答案：A得分： 9.0分\n",
    "二.多选题（共2题,18.0分）\n",
    "1\n",
    "（ ）是 Python 编程语言及其数值数学扩展包（ ）的可视化操作界面。它为利用通用的图形用户界面工具包，如 Tkinter, wxPython, Qt 或 GTK+ 向应用程序嵌入式绘图提供了应用程序接口。\n",
    "A、NumPy\n",
    "B、Matplotlib\n",
    "C、Scikit-learn\n",
    "正确答案： AB 我的答案：AB得分： 9.0分\n",
    "2\n",
    "Python编程语言受到广泛欢迎的原因：\n",
    "A、简单的语法\n",
    "B、内置AI项目库\n",
    "C、开源\n",
    "正确答案： ABC 我的答案：ABC得分： 9.0分\n",
    "三.判断题（共6题,54.0分）\n",
    "1\n",
    "符号主义致力于用计算机的符号操作来模拟人的认知过程其，实质就是模拟人的左脑抽象逻辑思维，通过研究人类认知系统的功能机理，用某种符号来描述人类的认知过程，并把这种符号输入到能处理符号的计算机中，从而模拟人类的认知过程，实现人工智能。\n",
    "我的答案：√ 得分： 9.0分正确答案：√\n",
    "2\n",
    "IBM的深蓝，主要通过博弈论算法，打败人类国际象棋冠军。\n",
    "我的答案：× 得分： 0.0分正确答案：√\n",
    "3\n",
    "人工神经网络是符号主义的典型代表性技术。\n",
    "我的答案：√ 得分： 0.0分正确答案：×\n",
    "4\n",
    "行为主义的贡献，主要在机器人控制系统方面。\n",
    "我的答案：√ 得分： 9.0分正确答案：√\n",
    "5\n",
    "123Python是合法变量。\n",
    "我的答案：× 得分： 9.0分正确答案：×\n",
    "6\n",
    "python与Python是相同的变量。\n",
    "我的答案：× 得分： 9.0分正确答案：×\n",
    "四.简答题（共2题,19.0分）\n",
    "1\n",
    "尝试利用python实现电脑随机生成1~100之间的整数，让用户来猜，猜错时，会提示猜的数字是大了还是小了，直到用户猜对为止，游戏结束。\n",
    "正确答案：\n",
    "\n",
    "import random computer=random.randint(1,100) while True: number=int(input(\"请输入100以内的整数：\")) if(number>computer): print(\"大了\") elif(number<computer): print(\"小了\") else: print(\"恭喜你赢了\") break\n",
    "我的答案：\n",
    "\n",
    "import random\n",
    "\n",
    "def guess_number():\n",
    "\n",
    "    true_num = random.randint(1, 100)\n",
    "\n",
    "    user_num = int(input(\"请输入一个整数:\"))\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while true_num != user_num:\n",
    "\n",
    "        if true_num > user_num:\n",
    "\n",
    "            print(\"太小了，请重新输入！\")\n",
    "\n",
    "        elif true_num < user_num:\n",
    "\n",
    "            print(\"太大了，请重新输入！\")\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        user_num = int(input(\"请输入一个整数：\"))\n",
    "\n",
    "    print(\"恭喜您，您猜对了！您一共猜了%d次\" % count)\n",
    "\n",
    "\n",
    "\n",
    "guess_number()\n",
    "\n",
    "2\n",
    "尝试利用python获取 100 以内的质数。\n",
    "正确答案：\n",
    "\n",
    "num=[]; i=2 for i in range(2,100): j=2 for j in range(2,i): if(i%j==0): break else: num.append(i) print(num)\n",
    "我的答案：\n",
    "\n",
    "num=[];\n",
    "\n",
    "i=2\n",
    "\n",
    "for i in range(2,100):\n",
    "\n",
    "   j=2\n",
    "\n",
    "   for j in range(2,i):\n",
    "\n",
    "      if(i%j==0):\n",
    "\n",
    "         break\n",
    "\n",
    "   else:\n",
    "\n",
    "      num.append(i)\n",
    "\n",
    "print(num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第五章：\n",
    "华东理工大学本科\n",
    "肖婷婷\n",
    "\n",
    "首页任务统计资料通知作业考试案例教学讨论\n",
    "人工智能基础  \n",
    "返回\n",
    "第5章\n",
    "   姓名：肖婷婷   班级：动181-3   成绩： 77.8分\n",
    "\n",
    "一.单选题（共3题,33.3分）\n",
    "1\n",
    "Lasso回归加入w的( )范数作为惩罚项,以确定系数中的数目较多的无效项\n",
    "\n",
    "A、\n",
    "L1\n",
    "\n",
    "B、\n",
    "L2\n",
    "\n",
    "C、\n",
    "L3\n",
    "\n",
    "正确答案： A 我的答案：A得分： 11.1分\n",
    "答案解析：\n",
    "\n",
    "2\n",
    "向量x=[1,2,3,4,-9,0]的L1范数是多少?( )\n",
    "A、1\n",
    "B、19\n",
    "C、6\n",
    "D、sqrt(111)\n",
    "正确答案： B 我的答案：B得分： 11.1分\n",
    "3\n",
    "使用带有 L1 正则化的 logistic 回归做二分类,其中 C 是正则化参数,w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时,下面哪个选项是正确的?( )\n",
    "A、第一个 w2 成了 0,接着 w1 也成了 0\n",
    "B、第一个 w1 成了 0,接着 w2 也成了 0\n",
    "C、w1 和 w2 同时成了 0\n",
    "D、即使在 C 成为大值之后,w1 和 w2 都不能成 0\n",
    "正确答案： B 我的答案：C得分： 0.0分\n",
    "二.多选题（共1题,11.1分）\n",
    "1\n",
    "关于L1正则和L2正则,下面说法正确的是( )\n",
    "A、L2范数可以防止过拟合,提升模型的泛化能力,但L1做不到这点\n",
    "B、L2正则化标识各个参数的平方的和的开方值\n",
    "C、L2正则化又叫“Lasso regularization”\n",
    "D、L1范数会使权值稀疏\n",
    "正确答案： BD 我的答案：BD得分： 11.1分\n",
    "三.判断题（共5题,55.6分）\n",
    "1\n",
    "线性回归方法可以适应弥散高的数据集。\n",
    "我的答案：√ 得分： 0.0分正确答案：×\n",
    "2\n",
    "逻辑回归实际上是一种基于概率来判断样本属于哪一类的分类方法。\n",
    "我的答案：√ 得分： 11.1分正确答案：√\n",
    "3\n",
    "使用ElasticNet方式,所得到的模型将像纯粹的Lasso回归一样稀疏,但同时具有与岭回归提供的正则化能力。\n",
    "我的答案：√ 得分： 11.1分正确答案：√\n",
    "4\n",
    "回归问题和分类问题都有可能发生过拟合。\n",
    "我的答案：√ 得分： 11.1分正确答案：√\n",
    "5\n",
    "给定n个数据点,如果其中一半用于训练,另一半用于测试,则训练误差和测试误差之间的差别会随着n的增加而减小。\n",
    "我的答案：√ 得分： 11.2分正确答案：√"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "华东理工大学本科\n",
    "肖婷婷\n",
    "\n",
    "首页任务统计资料通知作业考试案例教学讨论\n",
    "人工智能基础  \n",
    "返回\n",
    "第7章\n",
    "   姓名：肖婷婷   班级：动181-3   成绩： 78.1分\n",
    "\n",
    "一.单选题（共4题,28.4分）\n",
    "1\n",
    "假设你建立一个神经网络。 你决定将权重和偏差初始化为零。以下哪项陈述是正确的：\n",
    "A、即使在第一次迭代中，第一个隐藏层的神经元也会执行不同的计算， 他们的参数将以各自方式进行更新。\n",
    "B、第一个隐藏层中的每一个神经元都会计算出相同的结果，但是不同层的神经元会计算不同的结果。\n",
    "C、第一个隐藏层中的每个神经元将在第一次迭代中执行相同的计算。 但经过一次梯度下降迭代后，他们将会计算出不同的结果。\n",
    "D、第一个隐藏层中的每个神经元节点将执行相同的计算。 所以即使经过多次梯度下降迭代后，层中的每个神经元节点都会计算出与其他神经元节点相同的结果。\n",
    "正确答案： D 我的答案：D得分： 7.1分\n",
    "2\n",
    "你正在构建一个识别足球（y = 1）与篮球（y = 0）的二元分类器。你会使用哪一种激活函数用于输出层？\n",
    "A、tanh\n",
    "B、sigmoid\n",
    "C、Leaky ReLU\n",
    "D、ReLU\n",
    "正确答案： B 我的答案：B得分： 7.1分\n",
    "3\n",
    "假设有一个三分类问题，某个样本的标签为（1，0，0），模型的预测结果为（0.5，0.4，0.1），则交叉熵损失值（取自然对数结果）约等于：\n",
    "A、0.7\n",
    "B、0.8\n",
    "C、0.6\n",
    "D、0.5\n",
    "正确答案： A 我的答案：A得分： 7.1分\n",
    "4\n",
    "在网络训练时，loss在最初几个epoch没有下降，可能原因是： \n",
    "A、学习率过低\n",
    "B、正则参数过高\n",
    "C、陷入局部最小值\n",
    "D、以上都有可能\n",
    "正确答案： D 我的答案：D得分： 7.1分\n",
    "二.多选题（共3题,21.3分）\n",
    "1\n",
    "神经网络中常见的超参数有：\n",
    "A、梯度下降法迭代的步数\n",
    "B、学习率\n",
    "C、隐藏层数目\n",
    "D、正规化参数\n",
    "正确答案： ABCD 我的答案：ABCD得分： 7.1分\n",
    "2\n",
    "以下属于机器学习中用来防止过拟合的方法的是：\n",
    "A、Xiaver 初始化\n",
    "B、Dropout\n",
    "C、增加训练数据，比如数据增强、对抗网络生成数据\n",
    "D、增加神经网络层数\n",
    "正确答案： BC 我的答案：ABC得分： 0.0分\n",
    "3\n",
    "池化层在卷积神经网络中扮演了重要的角色，下列关于池化层的论述正确的有：\n",
    "A、池化操作可以扩大感受野\n",
    "B、池化操作可以实现数据的降维\n",
    "C、池化操作是一种线性变换\n",
    "D、池化操作具有平移不变性\n",
    "正确答案： ABD 我的答案：ABD得分： 7.1分\n",
    "三.判断题（共6题,42.6分）\n",
    "1\n",
    "在神经网络的训练中，我们一般将参数全部初始化为0。\n",
    "我的答案：× 得分： 7.1分正确答案：×\n",
    "2\n",
    "ReLU函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。\n",
    "我的答案：√ 得分： 7.1分正确答案：√\n",
    "3\n",
    "sigmoid函数不是关于原点中心对称的，这会导致之后的网络层的输出也不是零中心的，进而影响梯度下降运作。tanh激活函数解决了这个不足。\n",
    "我的答案：√ 得分： 7.1分正确答案：√\n",
    "4\n",
    "一般来说 batch Size 越大，其确定的下降方向越不准，引起训练loss震荡越大。\n",
    "我的答案：× 得分： 7.1分正确答案：×\n",
    "5\n",
    "RNN的短期记忆问题是由其梯度消失问题造成的。\n",
    "我的答案：× 得分： 0.0分正确答案：√\n",
    "6\n",
    "LSTM网络具有三个门，遗忘门，输入门，输出门。\n",
    "我的答案：√ 得分： 7.1分正确答案：√\n",
    "四.简答题（共1题,7.7分）\n",
    "1\n",
    "本题讨论波士顿房价问题。下载该数据集并将其直接使用文件名housing.csv保存到当前文件中。该数据集描述了波士顿郊区房屋的13个数值属性，并涉及对这些郊区房屋价格的建模（数千美元）。输入属性包括犯罪率，非零售营业面积比例，化学物质浓度等。因为所有输入和输出属性都是数字属性，并且有506个实例可以使用。使用Keras 和python的scikit-learn库来实现了对房价的回归预测。\n",
    "正确答案：\n",
    "\n",
    "我的答案：\n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "\n",
    "train_data -= mean\n",
    "\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "\n",
    "test_data /= std\n",
    "\n",
    "from keras import models\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "\n",
    "                         input_shape=(train_data.shape[1],)))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "\n",
    "num_val_samples = len(train_data) // k\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "\n",
    "    print('processing fold #', i)\n",
    "\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate(\n",
    "\n",
    "      [train_data[:i * num_val_samples],\n",
    "\n",
    "   train_data[(i + 1) * num_val_samples:]],\n",
    "\n",
    "  axis=0)\n",
    "\n",
    "    partial_train_targets = np.concatenate(\n",
    "\n",
    "      [train_targets[:i * num_val_samples],\n",
    "\n",
    "   train_targets[(i + 1) * num_val_samples:]],\n",
    "\n",
    "  axis=0)\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "\n",
    "            epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "\n",
    "    all_scores.append(val_mae)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "\n",
    "    print('processing fold #', i)\n",
    "\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate(\n",
    "\n",
    "      [train_data[:i * num_val_samples],\n",
    "\n",
    "   train_data[(i + 1) * num_val_samples:]],\n",
    "\n",
    "  axis=0)\n",
    "\n",
    "    partial_train_targets = np.concatenate(\n",
    "\n",
    "      [train_targets[:i * num_val_samples],\n",
    "\n",
    "   train_targets[(i + 1) * num_val_samples:]],\n",
    "\n",
    "  axis=0)\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "\n",
    "                      validation_data=(val_data, val_targets),\n",
    "\n",
    "  epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "\n",
    "    all_mae_histories.append(mae_history)\n",
    "\n",
    "average_mae_history = [\n",
    "\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "\n",
    "def smooth_curve(points, factor=0.9):\n",
    "\n",
    "    smoothed_points = []\n",
    "\n",
    "    for point in points:\n",
    "\n",
    "        if smoothed_points:\n",
    "\n",
    "            previous = smoothed_points[-1]\n",
    "\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "\n",
    "        else:\n",
    "\n",
    "            smoothed_points.append(point)\n",
    "\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.ylabel('Validation MAE')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.fit(train_data, train_targets,\n",
    "\n",
    "          epochs=80, batch_size=16, verbose=0)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "\n",
    "print(test_mae_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
