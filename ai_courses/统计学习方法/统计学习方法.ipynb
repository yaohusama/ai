{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "iris.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8e020f24b4ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34mb'Iris-setosa'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb'Iris-versicolor'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb'Iris-virginica'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iris.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miris_type\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python38\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python38\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python38\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: iris.txt not found."
     ]
    }
   ],
   "source": [
    "# skleran中集成了许多算法，其导入包的方式如下所示：\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "def iris_type(s):\n",
    "    it = {b'Iris-setosa': 0, b'Iris-versicolor':1, b'Iris-virginica': 2}\n",
    "    return it[s]\n",
    "data =np.loadtxt('iris.txt', dtype=float, delimiter=',', converters={4: iris_type})\n",
    "\n",
    "x, y = np.split(data, (4,), axis=1)\n",
    "x = x[:, :2]\n",
    "x_train, x_test, y_train, y_test =train_test_split(x, y, random_state=1, train_size=0.6)\n",
    "clf = svm.SVC(C=0.8, kernel='rbf',gamma=20,decision_function_shape='ovr')\n",
    "clf.fit(x_train, y_train.ravel())\n",
    "def show_accuracy(a, b, tip):\n",
    "    acc = a.ravel() == b.ravel()\n",
    "    print('%s 正确率:%.3f'%(tip, np.mean(acc)))\n",
    "print(clf.score(x_train, y_train))  # 精度\n",
    "y_hat =clf.predict(x_train)\n",
    "show_accuracy(y_hat,y_train, '训练集')\n",
    "print(clf.score(x_test, y_test))\n",
    "y_hat =clf.predict(x_test)\n",
    "show_accuracy(y_hat,y_test, '测试集')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "对数据进行预处理，去除列表中重复元素，以列表形式返回：\n",
    "deftext_parse(big_string):\n",
    "      token_list= big_string.split()\n",
    "      return[tok.lower() for tok in token_list if len(tok) > 2]\n",
    "defcreate_vocab_list(data_set):\n",
    "      vocab_set = set({})\n",
    "      for d in data_set:\n",
    "          vocab_set = vocab_set | set(d)\n",
    "      return list(vocab_set)\n",
    "统计每一文档（或邮件）在单词表中出现的次数，并以列表形式返回：\n",
    "defwords_to_vec(vocab_list, input_set):\n",
    "    return_vec = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)]+= 1\n",
    "    return return_vec\n",
    "朴素贝叶斯主程序如下：\n",
    "doc_list,class_list, x = [ ], [ ], [ ]\n",
    "for i inrange(1, 26):\n",
    "    # 读取第i篇垃圾文件，并以列表形式返回\n",
    "   word_list=text_parse(open('email/spam/{0}.txt'.format(i),encoding='ISO-8859-1').read())\n",
    "    doc_list.append(word_list)\n",
    "    class_list.append(1)\n",
    "    # 读取第i篇非垃圾文件，并以列表形式返回\n",
    "   word_list=text_parse(open('email/ham/{0}.txt'.format(i),encoding='ISO-8859-1').read())\n",
    "    doc_list.append(word_list)\n",
    "    class_list.append(0)\n",
    "将数据向量化并且分割数据为训练集和测试集：\n",
    "vocab_list =create_vocab_list(doc_list)\n",
    "for word_listin doc_list:\n",
    "    x.append(words_to_vec(vocab_list,word_list))\n",
    "x_train,x_test, y_train, y_test = train_test_split(x, class_list, test_size=0.25)\n",
    "x_train,x_test, y_train, y_test = np.array(x_train), np.array(x_test),np.array(y_train), np.array(y_test)\n",
    "训练模型：\n",
    "nb_model =MultinomialNB()\n",
    "nb_model.fit(x_train,y_train)\n",
    "此处采用了多项式贝叶斯分类器，此外sklearn中还提供先验为伯努利分布的BernoulliNB、先验为高斯分布的GaussianNB。多项式朴素贝叶斯的用法如下：\n",
    "MultinomialNB(alpha=1.0,fit_prior=True,class_prior=None)\n",
    "其中，alpha为浮点型可选参数，默认为1.0，其实就是添加拉普拉斯平滑，即式(6-17)中的，如果这个参数设置为0，就是不添加平滑。fit_prior为布尔型可选参数，默认为True，表示是否要考虑先验概率，如果是false，则所有的样本类别输出都有相同的类别先验概率，否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior，让MultinomialNB自己从训练集样本来计算先验概率。class_prior为可选参数，默认为None。\n",
    "测试模型效果：\n",
    "y_pred =nb_model.predict(x_test)\n",
    "# 输出预测情况\n",
    "print(\"正确值：{0}\".format(y_test))\n",
    "print(\"预测值：{0}\".format(y_pred))\n",
    "print(\"准确率：%f%%\" % (accuracy_score(y_test, y_pred)* 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
