{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "color-sampling",
   "metadata": {},
   "source": [
    "算法依赖于数据，在不同类型的数据上表现不同。而多数算法对数据的相关性，可靠性，有效性都相当敏感，为了提高数据质量，就需要进行预处理。\n",
    "\n",
    "预处理采用的措施也依据数据而不同，比如某些特征会呈现明显的周期或规律性，两个特征之间的强相关性，以及对异常值缺失值进行插值或者直接删除等的处理。\n",
    "\n",
    "缺失值会导致模型丢失有用数据，尤其是在缺失值较多的情况下，包含缺失值直接建模，对结果的准确性有很大的影响，而网络攻击预测的数据集，并无缺失值。\n",
    "\n",
    "异常值是对超过了理论上范围的异常量检测，或以3σ原则对符合正态分布的变量判断离群点，或使用箱型图分析，将上界以上，下届以下的点都作为离群点，排除其所在的记录。（数据过大，没处理）经常用的指标：均值，中位数，众数，极差，标准差，变异系数（离开均值的趋势），四分位数间距（衡量变异程度），这些衡量数据特征的指标见表（？）\n",
    "\n",
    "由于数据和时间无关联，则略去对周期性类似的规律分析。\n",
    "\n",
    "相关系数衡量两个变量线性关联的程度，大于0.8通常作为高度相关，可以删去其中一列，同时也提供了对于异名同义的解决方法，一定程度上减小了数据量，兼保证了数据的信息损失较小。\n",
    "\n",
    "标准化消除了量纲影响，以免偏向于数值较大的特征。\n",
    "\n",
    "归一化，则是将数据都在保留了相对大小关系的前提下，将某个特征的和转为1，对后续处理有所帮助。\n",
    "\n",
    "特征多，维度大的情况下，如果某几个特征提供了高达八成的信息量或者更多，则可以将维度降低到更低维度，以减少计算量，提高性能，且不丢失过多信息。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "def del_nul():\n",
    "    source_file='kddcup.data_10_percent_corrected'\n",
    "    handled_file='kddcup.data_10_percent_corrected.csv'\n",
    "    data_file=open(handled_file,'w',newline='')     #python3.x中添加newline=''这一参数使写入的文件没有多余的空行\n",
    "    with open(source_file,'r') as data_source:\n",
    "        csv_reader=csv.reader(data_source)\n",
    "        csv_writer=csv.writer(data_file)\n",
    "        count=0   #记录数据的行数，初始化为0\n",
    "        for row in csv_reader:\n",
    "            row=np.array(row)\n",
    "            flag=0\n",
    "            for col in row:\n",
    "                if len(col) ==0:\n",
    "                    flag=1\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            csv_writer.writerow(row)\n",
    "            \n",
    "            count+=1\n",
    "            #输出每行数据中所修改后的状态\n",
    "        print(count)\n",
    "        data_file.close()\n",
    "del_nul()#无缺失值，除了删除缺失值，还可以进行拉格朗日插值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-saint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import time\n",
    "import re\n",
    "\n",
    "def classify(input_vct, data_set):\n",
    "    data_set_size = data_set.shape[0]\n",
    "    diff_mat = np.tile(input_vct, (data_set_size, 1)) - \\\n",
    "        data_set  # 扩充input_vct到与data_set同型并相减\n",
    "    sq_diff_mat = diff_mat**2  # 矩阵中每个元素都平方\n",
    "    distance = sq_diff_mat.sum(axis=1)**0.5  # 每行相加求和并开平方根\n",
    "    return distance.min(axis=0)  # 返回最小距离\n",
    "\n",
    "\n",
    "def LDA(X, y):\n",
    "    X1 = np.array([X[i] for i in range(len(X)) if y[i] == 1])\n",
    "    X2 = np.array([X[i] for i in range(len(X)) if y[i] == 2])\n",
    "\n",
    "    len1 = len(X1)\n",
    "    len2 = len(X2)\n",
    "\n",
    "    mju1 = np.mean(X1, axis=0)  # 求中心点\n",
    "    mju2 = np.mean(X2, axis=0)\n",
    "\n",
    "    cov1 = np.dot((X1 - mju1).T, (X1 - mju1))\n",
    "    cov2 = np.dot((X2 - mju2).T, (X2 - mju2))\n",
    "    Sw = cov1 + cov2\n",
    "\n",
    "    w = np.dot(np.linalg.pinv(Sw),\n",
    "               (mju1 - mju2).reshape((len(mju1), 1)))  # 计算w\n",
    "    X1_new = np.dot(X1, w)\n",
    "    X2_new = np.dot(X2, w)\n",
    "    y1_new = [1 for i in range(len1)]\n",
    "    y2_new = [2 for i in range(len2)]\n",
    "\n",
    "    X_new = np.concatenate((X1_new, X2_new), axis=0)\n",
    "    y_new = np.concatenate((y1_new, y2_new), axis=0)\n",
    "\n",
    "    return X_new, y_new\n",
    "\n",
    "\n",
    "def normal_or_not(x):  # 辅助函数\n",
    "#     if (re.match(x,'normal')) is None:\n",
    "    if x=='normal.':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def preprocess(file_name, tag):\n",
    "    df = pd.read_csv(file_name, header=None)#查看pandas官方文档发现，read_csv读取时会自动识别表头，数据有表头时不能设置header为空（默认读取第一行，即header=0)；数据无表头时，若不设置header，第一行数据会被视为表头，应传入names参数设置表头名称或设置header=None。\n",
    "\n",
    "    df[41] = df[41].apply(lambda x: normal_or_not(x))  # 转换后提取最后一列\n",
    "    class_label = np.array(df[41])\n",
    "    print(class_label)\n",
    "    data = df.drop(columns=41)\n",
    "\n",
    "    data = np.array(pd.get_dummies(data))#onehot编码，会自动将是字符串的列转为onehot编码，如果列名是数字可能不会再扩充充，而是把那一列转为数字型\n",
    "    np.savetxt(str(tag)+'onehot.csv', data, delimiter=',')\n",
    "    print(file_name+'完成nominal数据转换')\n",
    "\n",
    "    #data, class_label = LDA(data, class_label)\n",
    "    #print(file_name+'完成数据降维')\n",
    "    \n",
    "    '''\n",
    "    model = SMOTEENN(random_state=0)\n",
    "    data, class_label = model.fit_sample(data, class_label)\n",
    "    print(file_name+'完成上下采样处理')\n",
    "    '''\n",
    "    model = SMOTE()#采样\n",
    "    data, class_label = model.fit_sample(data, class_label)\n",
    "    print(file_name+'完成上下采样处理')\n",
    "\n",
    "    mm = MinMaxScaler()\n",
    "    data = mm.fit_transform(data)\n",
    "    print(file_name+'完成数据归一化')\n",
    "\n",
    "    np.savetxt(str(tag)+'data1.csv', data, delimiter=',')\n",
    "    np.savetxt(str(tag)+'label1.csv', class_label, delimiter=',')\n",
    "    \n",
    "    return data, class_label\n",
    "\n",
    "\n",
    "def roc(data_set):\n",
    "    normal = 0\n",
    "    data_set_size = data_set.shape[1]\n",
    "    roc_rate = np.zeros((2, data_set_size))\n",
    "    for i in trange(data_set_size):\n",
    "        if data_set[2][i] == 1:#2代表？\n",
    "            normal += 1\n",
    "    abnormal = data_set_size - normal\n",
    "    max_dis = data_set[1].max()\n",
    "    for j in range(1000):\n",
    "        threshold = max_dis / 1000 * j\n",
    "        normal1 = 0\n",
    "        abnormal1 = 0\n",
    "        for k in range(data_set_size):\n",
    "            if data_set[1][k] > threshold and data_set[2][k] == 1:\n",
    "                normal1 += 1\n",
    "            if data_set[1][k] > threshold and data_set[2][k] == 2:\n",
    "                abnormal1 += 1\n",
    "        roc_rate[0][j] = normal1 / normal  # 阈值以上正常点/全体正常的点\n",
    "        roc_rate[1][j] = abnormal1 / abnormal  # 阈值以上异常点/全体异常点\n",
    "    return roc_rate\n",
    "\n",
    "\n",
    "def test(training_filename, test_filename):\n",
    "    \n",
    "    start = time.time()\n",
    "    training_mat, training_label = preprocess(training_filename, 'train1_')\n",
    "    end = time.time()\n",
    "    \n",
    "    print('训练集预处理完毕,用时', end-start, '秒')\n",
    "    start = time.time()\n",
    "    test_mat, test_label = preprocess(test_filename, 'test1_')\n",
    "    end = time.time()\n",
    "    print('测试集预处理完毕,用时', end-start, '秒')\n",
    "    \n",
    "    '''\n",
    "    training_mat=np.array(pd.read_csv('1data.csv',header=None))\n",
    "    training_label=np.array(pd.read_csv('1label.csv',header=None))\n",
    "    test_mat=np.array(pd.read_csv('2data.csv',header=None))\n",
    "    test_label=np.array(pd.read_csv('2label.csv',header=None))\n",
    "    \n",
    "    test_size = test_mat.shape[0]\n",
    "    result = np.zeros((test_size, 3))\n",
    "    for i in trange(test_size):\n",
    "        # 序号， 最小欧氏距离， 测试集数据类别\n",
    "       result[i] = i + 1, classify(test_mat[i], training_mat), test_label[i]\n",
    "    print('欧氏距离计算完毕，开始作图')\n",
    "    result = np.transpose(result)  # 矩阵转置\n",
    "    plt.figure(1)\n",
    "    plt.scatter(result[0], result[1], c=result[2],\n",
    "                edgecolors='None', s=1, alpha=1)\n",
    "    # 图1 散点图：横轴为序号，纵轴为最小欧氏距离，点中心颜色根据测试集数据类别而定， 点外围无颜色，点大小为最小1，灰度为最大1\n",
    "    roc_rate = roc(result)\n",
    "    plt.figure(2)\n",
    "    plt.scatter(roc_rate[0], roc_rate[1], edgecolors='None', s=1, alpha=1)\n",
    "    # 图2 ROC曲线， 横轴误报率，即阈值以上正常点/全体正常的点；纵轴检测率，即阈值以上异常点/全体异常点\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test('kddcup.data_10_percent_corrected.csv', 'kddcup.data.corrected.csv')#直接右击重命名加后缀不容易出现编码问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normalize=pd.read_csv('train_data.csv',encoding='utf-8')\n",
    "# train_normalize=(train_normalize-train_normalize.mean())/(train_normalize.std())\n",
    "statistics = train_normalize.describe()  # 保存基本统计量\n",
    "\n",
    "statistics.loc['range'] = statistics.loc['max']-statistics.loc['min']  # 极差\n",
    "statistics.loc['var'] = statistics.loc['std']/statistics.loc['mean']  # 变异系数\n",
    "statistics.loc['dis'] = statistics.loc['75%']-statistics.loc['25%']  # 四分位数间距\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm#SVM和朴素贝叶斯模板训练加预测\n",
    "from sklearn import datasets\t#自带数据集\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "train_data_='train_data.csv'\n",
    "train_label_='train_label.csv'\n",
    "test_data_='test_data.csv'\n",
    "test_label_='test_label.csv'\n",
    "\n",
    "train_data=np.array(pd.read_csv(train_data_,header=None))\n",
    "train_label=np.array(pd.read_csv(train_label_,header=None))\n",
    "test_data=np.array(pd.read_csv(test_data_,header=None))\n",
    "test_label=np.array(pd.read_csv(test_label_,header=None))\n",
    "\n",
    "'''\n",
    "X,y = datasets.load_breast_cancer(return_X_y=True)\n",
    "df_X=pd.DataFrame(X)\n",
    "df_y=pd.DataFrame(y)\n",
    "df_X['y']=df_y\n",
    "df=shuffle(df_X)\n",
    "df_y=df['y']\n",
    "df=df.drop(columns='y')\n",
    "\n",
    "X=np.array(df)\n",
    "y=np.array(df_y)\n",
    "\n",
    "train_data=X[:400]\n",
    "train_label=y[:400]\n",
    "test_data=X[400:]\n",
    "test_label=y[400:]'''\n",
    "\n",
    "\n",
    "gnb=GaussianNB()\n",
    "\n",
    "start=time.time()\n",
    "gnb.fit(train_data,train_label.ravel())\n",
    "end=time.time()\n",
    "print('naive bayes分类用时',end-start,'秒')\n",
    "\n",
    "start=time.time()\n",
    "training_score=gnb.score(train_data,train_label)\n",
    "end=time.time()\n",
    "print(\"训练集得分：\",training_score,'用时：',end-start,'秒')\n",
    "\n",
    "start=time.time()\n",
    "training_score=gnb.score(test_data,test_label)\n",
    "end=time.time()\n",
    "print(\"测试集得分：\",training_score,'用时：',end-start,'秒')\n",
    "'''\n",
    "classifier=svm.SVC(C=2,kernel='linear',gamma=10,decision_function_shape='ovr') # ovr:一对多策略\n",
    "\n",
    "start=time.time()\n",
    "classifier.fit(train_data,train_label.ravel())\n",
    "end=time.time()\n",
    "print('svm分类用时',end-start,'秒')\n",
    "\n",
    "start=time.time()\n",
    "training_score=classifier.score(train_data,train_label)\n",
    "end=time.time()\n",
    "print(\"训练集得分：\",training_score,'用时：',end-start,'秒')\n",
    "\n",
    "start=time.time()\n",
    "training_score=classifier.score(test_data,test_label)\n",
    "end=time.time()\n",
    "print(\"测试集得分：\",training_score,'用时：',end-start,'秒')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "mat=train_normalize.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_part=[]\n",
    "mat.columns=range(118)\n",
    "for i in range(1,118):\n",
    "    for j in range(1,118):\n",
    "        if mat.loc[i,j]>=0.9:\n",
    "#              strong_part.append(i)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-firewall",
   "metadata": {},
   "source": [
    "# 以下是学习内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm,trange#tqdm使用，打印进度条\n",
    "import time\n",
    " \n",
    "a = [1,2,3]\n",
    " \n",
    "for i in tqdm(a):\n",
    "    print('打印a[%d]：'%(i-1),i)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-magazine",
   "metadata": {},
   "source": [
    "# SMOTE算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-booking",
   "metadata": {},
   "source": [
    "SMOTE（Synthetic Minority Oversampling Technique），合成少数类过采样技术．它是基于随机过采样算法的一种改进方案，由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使得模型学习到的信息过于特别(Specific)而不够泛化(General)，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中，具体如下图所示，算法流程如下。\n",
    "\n",
    "(1)对于少数类中每一个样本x，以欧氏距离为标准计算它到少数类样本集中所有样本的距离，得到其k近邻。\n",
    "\n",
    "(2)根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为xn。\n",
    "\n",
    "(3)对于每一个随机选出的近邻xn，分别与原样本按照如下的公式构建新的样本。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "smote算法的伪代码如下：\n",
    "\n",
    "\n",
    "\n",
    "SMOTE算法的缺陷\n",
    "\n",
    "该算法主要存在两方面的问题:一是在近邻选择时,存在一定的盲目性。从上面的算法流程可以看出,在算法执行过程中,需要确定K值,即选择多少个近邻样本,这需要用户自行解决。从K值的定义可以看出,K值的下限是M值(M值为从K个近邻中随机挑选出的近邻样本的个数,且有M< K),M的大小可以根据负类样本数量、正类样本数量和数据集最后需要达到的平衡率决定。但K值的上限没有办法确定,只能根据具体的数据集去反复测试。因此如何确定K值,才能使算法达到最优这是未知的。\n",
    "\n",
    "另外,该算法无法克服非平衡数据集的数据分布问题,容易产生分布边缘化问题。由于负类样本的分布决定了其可选择的近邻,如果一个负类样本处在负类样本集的分布边缘,则由此负类样本和相邻样本产生的“人造”样本也会处在这个边缘,且会越来越边缘化,从而模糊了正类样本和负类样本的边界,而且使边界变得越来越模糊。这种边界模糊性,虽然使数据集的平衡性得到了改善,但加大了分类算法进行分类的难度．\n",
    "\n",
    "针对SMOTE算法的进一步改进\n",
    "\n",
    "针对SMOTE算法存在的边缘化和盲目性等问题,很多人纷纷提出了新的改进办法,在一定程度上改进了算法的性能,但还存在许多需要解决的问题。\n",
    "\n",
    "Han等人Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning在SMOTE算法基础上进行了改进,提出了Borderhne.SMOTE算法,解决了生成样本重叠(Overlapping)的问题该算法在运行的过程中,查找一个适当的区域,该区域可以较好地反应数据集的性质,然后在该区域内进行插值,以使新增加的“人造”样本更有效。这个适当的区域一般由经验给定,因此算法在执行的过程中有一定的局限性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-feedback",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
